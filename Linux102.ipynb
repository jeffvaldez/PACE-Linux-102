{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"font-size:22pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    PACE Linux 102: Efficient Workflow with Command Line Utilities\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <a href = \"mailto: ajezghani3@gatech.edu\"><b>Aaron Jezghani, PhD</b></a><br>\n",
    "    <a href = \"https://pace.gatech.edu\" target = \"_blank\"><b>PACE, Georgia Tech</b></a>\n",
    "</center>\n",
    "\n",
    "\n",
    "PACE’s Linux 102: Efficient Workflow with Command Line Utilities builds upon the Linux 101 content to demonstrate extended features to improve user knowledge and workflow using command line utilities in a Hands-On course. Topics covered include the use of built-in job control to manage multiple processes, filtering and stream-processing utilities for lightweight data processing, advanced I/O options, and data compression utilities. The content is focused on example usage, and is intended to provide a template for users to adopt in their own workflow.\n",
    "\n",
    "#### This Course is Being Taught Online Exclusively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=4 color=B3A369><u><b><i>Why</i> are we running bash through a Jupyter notebook?</b></u></font>\n",
    "\n",
    "Traditionally, this course would be taught using a slide-deck and a terminal for the hands-on components. While it is true that the terminal has more overall functionality (as you can run interactive shells), a Jupyter Notebook is used to integrate the course content and hands-on exercises into a single document, while maintaining a record of all output for your review. The examples and exercises for this class have been designed so that they can fully run within this notebook.\n",
    "\n",
    "<font size=4 color=B3A369><u><b>Connect to PACE-ICE</b></u></font>\n",
    "\n",
    "- Connect to the GT VPN and then login to <b>ondemand-pace-ice.pace.gatech.edu</b>. Open OnDemand is a gateway developed by Ohio Supercomputing Center and adopted at HPC centers around the world. Through your browser, you can launch interactive applications such as a terminal shell or Jupyter notebooks.\n",
    "- You can connect to PACE-ICE via ssh. On Mac or Linux, a terminal window provides access to ssh, while on Windows 10 release 18.03 and newer, it can be accessed via PowerShell (which is already installed!). PuTTY and other ssh software can be used, but you should consult the manual to add port-forwarding to an existing session, as the displayed instructions will not work.\n",
    "- If you have an older version of Windows or your computer has a heavily restricted firewall, you may wish to use VLab (http://mycloud.gatech.edu), which offers access to Windows machines on Georgia Tech's Virtual Lab. \n",
    "\n",
    "- Log in to PACE-ICE via the headnode: \n",
    "    - `ssh USERNAME@login-pace-ice.pace.gatech.edu`. \n",
    "    - Replace USERNAME with your username. \n",
    "    - Enter your password when prompted. No asterisks will appear, but type your full password, then hit Enter.\n",
    "\n",
    "<font size=4 color=B3A369><u><b>Copy workshop files</b></u></font>\n",
    "- Download the workshop repo from Github:\n",
    "    - `git clone https://github.com/apjez/PACE-Linux-102.git`\n",
    "- Change into the Linux102 directory, where you should have a Jupyter notebook file and some other content\n",
    "\n",
    "\n",
    "<font size=4 color=B3A369><u><b>Installing Anaconda, with Python 3, bash_kernel, and Jupyter included, on your laptop</b></u></font>\n",
    "\n",
    "- The `bash_kernel` allows you to execute bash commands within your environment without the use of the %%bash magic command so that each code block reflects native bash (although the magic command has real benefits too - you should check it out!)\n",
    "\n",
    "- For those without PACE accounts, this will allow you to use Jupyter Notebooks with `bash_kernel` in the future. Even those with PACE access may find using a local installation to be a convenient option at times. \n",
    "\n",
    "- Download from www.anaconda.com, for Mac/Linux/Windows\n",
    "\n",
    "- To install bash_kernel, simply run the following commands after you have installed Anaconda:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge bash_kernel #installs bash_kernel\n",
    "python -m bash_kernel.install #makes bash_kernel available for jupyter notebook\n",
    "```\n",
    "\n",
    "- Load Jupyter from the Anaconda Navigator, or type `jupyter notebook` on the command line (except on Windows). \n",
    "\n",
    "- When you finish, end the Jupyter session by pressing `Ctrl-c` in the terminal where you started it (except on Windows), or click **Quit** in the top right of the main Jupyter window. (orange arrow below)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:24pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    Stream Manipulation\n",
    "</p>\n",
    "\n",
    "When used correctly, Bash can be extremely powerful for language and data processing, especially through stream manipulation. A number of utilities exist to quickly perform some simple data-handling tasks:\n",
    "- `tr <set1> <set2>` can be used to translate characters from _set1_ to _set2_\n",
    "    - The `-d` option is used to delete characters (don't provide <i>set2</i>)\n",
    "    - The `-c` option inverts the character set to match\n",
    "    - The `-s` option is used to \"squeeze\" the whitespace between text fields in formatted text\n",
    "- `sort` is used to sort a text stream or file\n",
    "    - The `-n` option is used to do a numerical sort\n",
    "    - The `-r` options reverses the sorted order\n",
    "    - The `-m` option merges presorted input files\n",
    "    - The `-u` option only prints one occurrence of each item in the sorted list\n",
    "    - The `-k` option allows you to define the key (column) to sort by\n",
    "    - The `-t` option is used to indicate the field delimiter\n",
    "- `uniq` removes duplicate lines from a sorted file\n",
    "    - Without any options, will simply return the first occurrence of each line\n",
    "    - The `-c` option can be used to count the number of occurrences of each line\n",
    "- `head` is used to print from the beginning of a file\n",
    "    - The `-n=<#>` option is used to specify the number of lines to print\n",
    "    - The `-c=<#>` option is used to specify the number of bytes to print\n",
    "    - The `<#>` can be preceded by a `-` rather than `=` (e.g. `-n-10`) to print all but the last `<#>` lines or bytes\n",
    "- `tail` is used to print from the end of a file\n",
    "    - The `-n=<#>` option is used to specify the number of lines to print\n",
    "    - The `-c=<#>` option is used to specify the number of bytes to print\n",
    "    - The `<#>` can be preceded by a `+` rather than `=` (e.g. `-n+10`) to print everything after `<#>` lines or bytes\n",
    "- `cut` is used to extract a specific field from a file\n",
    "    - The `-f<#>[,<#>,<#>,...]` option is used to specify which field(s) to cut\n",
    "    - The `-d<C>` option to indicate the character used to delimit fields in the file\n",
    "- `paste` is used to merge lines of files\n",
    "    - The `-d=<LIST>` option specifies a list of delimeters, to be cycled through on each line, to delimit each field\n",
    "    - The `-s` option is used to combine lines from each file separately, rather than merging the respective lines\n",
    "    \n",
    "<font color=377117>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Example 2: Filtering Command Output\n",
    "</p>\n",
    "\n",
    "- The first line shows the familiar output for the details of the pace-ice queue:  \n",
    "  <i>the head command is used to only print the first 11 lines of output</i>\n",
    "- The second line does the same, but adds a translate command to filter the output:  \n",
    "  <i>the translate command is used to turn all '/' and ' ' characters into '|' to highlight the effect of formatted text on fields</i>\n",
    "- The third line utilizes multiple filters to print an ordered list of the per-node memory utilization percent in the queue\n",
    "  <i>the tail command is used to print only compute node lines, translate is used with squeeze to reduce all whitespace to a single character, the 10th field is cut, a descending numerical sort is implemented, and finally the counts for each unique value are ascertained\n",
    "    \n",
    "<b>Execute each line, and note what each command does</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pace-check-queue pace-ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pace-check-queue pace-ice | tail -n+12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pace-check-queue pace-ice | tail -n+12 | tr -s '/' ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pace-check-queue pace-ice | tail -n+12 | tr -s '/' ' '  | cut -f7 -d' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pace-check-queue pace-ice | tail -n+12 | tr -s '/' ' '  | cut -f7 -d' ' | sort -nru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pace-check-queue pace-ice | tail -n+12 | tr -s '/' ' '  | cut -f7 -d' ' | sort -nr | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>\n",
    "    Shell Parameter Expansion\n",
    "</b></font>: light-weight variable manipulation</u></font>\n",
    "\n",
    "Beyond arithmetic expansion (`$((...))`) and command substitution (`$(<cmd> <options>)`), the `$` character also introduces <b>parameter expansion</b>. Substrings can be selected by index/pattern, variables can be indirectly references, string lengths can be measured, and strings can be modified - all using parameter expansion. The general syntax of parameter expansion uses some combination of `${...}`, `PARAMETER` (the value being modified - cannot be an expansion or pattern), `WORD` (the pattern explaining what to modify), and offset/length:\n",
    "- `${!PARAMETER}`: indirectly references the variable pointed to by parameter\n",
    "- `${#PARAMETER}`: returns the length of `PARAMETER`\n",
    "- `${PARAMETER:offset:length}`: as in Python, returns the substring of length `length` starting at index `offset` (zero-indexed, inclusive bound). If offset is less than 0, returns the substring starting at the index corresponding to `offset` characters from the end of `PARAMETER` (inclusive bound - note that the syntax here is `${PARAMETER: -offset:length}`: to avoid issues with the `:-` operator); if length is less than zero, returns the substring through `length` characters from the end of `PARAMETER` (exclusive bound)\n",
    "- `${PARAMETER%WORD}`: Removes `WORD` from the end of `PARAMETER` (note that `WORD` can be a globbing pattern)\n",
    "- `${PARAMETER%%WORD}`: Removes the longest match for `WORD` from the end of `PARAMETER` (note that `WORD` can be a globbing pattern)\n",
    "- `${PARAMETER#WORD}`: Removes `WORD` from the start of `PARAMETER` (note that `WORD` can be a globbing pattern)\n",
    "- `${PARAMETER##WORD}`: Rmoves the longest match for `WORD` from the start of `PARAMETER` (note that `WORD` can be a globbing pattern)\n",
    "- `${PARAMETER^WORD}`: Changes first character of `PARAMETER` to uppercase if it matches `WORD`, which can be a globbing pattern.\n",
    "- `${PARAMETER^^WORD}`: Changes all characters of `PARAMETER` to uppercase if they match `WORD`, which may be a globbing pattern.\n",
    "- `${PARAMETER,WORD}`: Changes first character of `PARAMETER` to uppercase if it matches `WORD`, which can be a globbing pattern.\n",
    "- `${PARAMETER,,WORD}`: Changes all characters of `PARAMETER` to uppercase if they match `WORD`, which may be a globbing pattern.\n",
    "- `${PARAMETER/WORD/NEWWORD}`: Replaces first occurrence of `WORD` with `NEWWORD` in `PARAMETER` (note that `WORD` can be a globbing pattern, but `NEWWORD` cannot)\n",
    "- `${PARAMETER//WORD/NEWWORD}`: Replaces all occurrences of `WORD` with `NEWWORD` in `PARAMETER` (note that `WORD` can be a globbing pattern, but `NEWWORD` cannot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $PBS_JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUST_ANOTHER_VAR=\"PBS_JOBID\"\n",
    "echo ${JUST_ANOTHER_VAR}\n",
    "echo ${!JUST_ANOTHER_VAR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${#PBS_JOBID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${PBS_JOBID:7}\n",
    "echo ${PBS_JOBID:7:5}\n",
    "echo ${PBS_JOBID:-10}\n",
    "echo ${PBS_JOBID: -10}\n",
    "echo ${PBS_JOBID: -10:3}\n",
    "echo ${PBS_JOBID: -10:-6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${PBS_JOBID%.*}\n",
    "echo ${PBS_JOBID%%.*}\n",
    "echo ${PBS_JOBID%.p*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${PBS_JOBID#*-}\n",
    "echo ${PBS_JOBID##*-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${PBS_JOBID^*}\n",
    "echo ${PBS_JOBID^s}\n",
    "echo ${PBS_JOBID^^*}\n",
    "echo ${PBS_JOBID^^[se]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "echo ${JUST_ANOTHER_VAR,*}\n",
    "echo ${JUST_ANOTHER_VAR,,*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${PBS_JOBID/[ec]/*}\n",
    "echo ${PBS_JOBID//[ec]/*}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>\n",
    "    Regular Expressions\n",
    "</b></font>: character pattern descriptions</u></font>\n",
    "\n",
    "In addition to the above filter utilities, regular expressions can be used for stream manipulation. A regular expression (abbreviated as regex or regexp) is a pattern that describes a sequence of characters. Programs such as `sed`, `awk`, and `grep` use them to perform operations and search for general patterns, such as finding and stripping email addresses from large volumes of user data. There are entire text books written about the use of regular expressions, but some of the general patterns are listed here:\n",
    "\n",
    "|Operator | Effect                       |Example |\n",
    "|:---     | :---                         |:---    |\n",
    "|` . `    |Matches any single character except new line. | **ab.** matches **abc**, **abC**, **abz**, **ab5**, **ab$**, **...**|\n",
    "|`?`    |Matches the preceding item 0 or 1 times.|**ab?c** matches **ac** and **abc**, but not **abbc**|\n",
    "|`*`    |Matches the preceding item 0 or more times.|**ab*c** matches **ac**, **abc**, **abbc**, **abbbc**, **...**|\n",
    "|`+`    |Matches the preceding item 1 or more times.|**ab+c** matches **abc** and **abbc**, but not **ac**|\n",
    "|`{n}`|Matches the preceding item exactly _n_ times|**ab{2}c** matches **abbc** but not **abc** or **abbbc**|\n",
    "|`{n,}`|Matches the preceding item _n_ or more times|**ab{2,}c** matches **abbc**, **abbbc**, **...** but not **abc**|\n",
    "|`{n,m}`|Matches the preceding itme at least _n_ times, but not more than _m_ times|**ab{2,3}c** matches **abbc** and **abbbc**, but not **abc** or **abbbbc**|\n",
    "|`[ ]`|Matches any single character contained in a set enclosed by `[` and `]`; ranges can be specified with `-`|**a[bBr-u]c** matches **abc**, **aBc**, **arc**, **asc**, **atc**, and **auc**|\n",
    "|`-[ ]` in `[ ]`|Character set subtraction|**a[a-z-[ac-z]]c** matches **abc** (`(a-z)-(a+c-z)=b`)|\n",
    "|`[^ ]`|Matches any single character not included in set enclosed by `[` and `]`|**a[^c-z]c** matches **aac**, **abc**, **aCc**, **a3c**, **a^c**, **...**|\n",
    "|`( )`|Matches a group of characters for extracting a substring or using a backreference|**a(abc)+c** matches **aabcc**, **aabcabcc**, **...** but not **aabc**|\n",
    "|`^` |Start of a line or string|**^a** matches **abc 123**, but not **123 abc** |\n",
    "|`$` |End of a line or string|**a\\$** matches **bababa**, but not **ababab** |\n",
    "\n",
    "_Note: in basic regular expressions the metacharacters `?`, `+`, `{`, `|`, `(`, and `)` lose their special meaning; \n",
    "<br>instead use the backslashed versions `\\?`, `\\+`, `\\{`, `\\|`, `\\(`, and `\\)`_\n",
    "\n",
    "\n",
    "\n",
    "In addition to the above regular expression operators to describe patterns, certain classes, which can describe general types of characters, are defined by the POSIX standard. Since each class is actually a list of characters, it should be enclosed in square brackets (i.e. `[[:alpha:]]` not `[:alpha:]`)\n",
    "\n",
    "|POSIX Class | Bracket Expression| Meaning |\n",
    "|:---  |:--- |:---|\n",
    "|`[:alnum:]`|`[A-Za-z0-9]`|upper- and lowercase letters, digits|\n",
    "|`[:alpha:]`|`[A-Za-z]`|upper- and lowercase letters|\n",
    "|`[:ascii:]`|`[\\x00-\\x7F]`|ASCII characters|\n",
    "|`[:blank:]`|`[ \\t]`|space and TAB characters only|\n",
    "|`[:cntrl:]`|`[\\x00-\\x1F\\x7F]`|control characters|\n",
    "|`[:digit:]`|`[0-9`]|digits|\n",
    "|`[:graph:]`|`[^[:cntrl:]]`|graphic characters (all characters which have graphic representation|\n",
    "|`[:lower:]`|`[a-z]`|lowercase letters|\n",
    "|`[:print:]`|`[[:graph:] ]`|graphic characters and space|\n",
    "|`[:punct:]`|`[!\"#$%&'()*+,-.\\/:;<=>?@‘[]^_{\\|}~]`|punctuation and symbols|\n",
    "|`[:space:]`|`[ \\t\\r\\n\\v\\f]`|all whitespace characters, including line breaks|\n",
    "|`[:upper:]`|`[A-Z]`|uppercase letters|\n",
    "|`[:word:]`|`[A-Zza-z0-9_]`|upper- and lowercase letters, digits, and underscores|\n",
    "|`[:xdigit:]`|`[A-Fa-f0-9]`|hexadecimal digits|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>grep</b></font>: _g/re/p_ (<b>g</b>lobally search a <b>r</b>egular <b>e</b>xpression and <b>p</b>rint)</u></font>\n",
    "\n",
    "As mentioned in Linux 101, grep is a great utility for searching for and printing matching text within files or streams. Given the name, it shouldn't be surprising that `grep` can also be used to look for regular expressions too!\n",
    "\n",
    "Here are some of the more useful options to use with grep:\n",
    "\n",
    "|Option|Meaning|\n",
    "|:---|:---|\n",
    "|`-E`|Interpret PATTERN as an extended regular expression|\n",
    "|`-f FILE`|Obtain patterns from `FILE`, one per line. The empty file contains zero patterns, and therefore matches nothing.|\n",
    "|`-i`|Ignore case distrinctions in both the PATTERN and the input files.|\n",
    "|`-c`|Suppress normal output; instead print a count of matching lines for each input file. With the `-v` option, count non-matching lines.\n",
    "|`-l`|Suppress normal output; instead print the name of each input file from which output would normally have been printed. The scanning will stop on the first match.|\n",
    "|`-o`|Print only the matched (non-empty) parts of a matching line, with each such part on a separate output line.|\n",
    "|`-v`|Invert the sense of matching, to select non-matching lines.|\n",
    "|`-m <#>`|Only print *#* matching lines\n",
    "|`-n`|Prefix each line of output with the 1-based line number within its input file.|\n",
    "|`-q`|Quit immediately and return exit status 0 if a match is found; else return 1. (useful for conditionals!)\n",
    "|`-R`|Search recursively for expression.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -n 'resource'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -q 'resource'\n",
    "echo $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -q 'REsource'\n",
    "echo $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -m 3 'resource'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -c -i 'resource'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep 'resources_used.cput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep 'resources_used.walltime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -E 'resources_used.cput|resources_used.walltime'\n",
    "qstat -f $PBS_JOBID | egrep 'resources_used.cput|resources_used.walltime'\n",
    "qstat -f $PBS_JOBID | grep 'resources_used.cput\\|resources_used.walltime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep 'resources_used.*[0-9]*:[0-9]*:[0-9]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep 'resources_used.+[0-9]+:[0-9]+:[0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | grep -E 'resources_used.*([0-9]+):\\1:\\1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | egrep 'resources_used.+[0-9]+:[0-9]+:[0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | egrep '.*use.* [0-9:]{8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369>**sed**</font>: **s**tream **ed**itor</u></font>\n",
    "\n",
    "sed is another extremely powerful regular expression utility. Typically, it is used to search and replace in more robust fashions than `tr`, but it can also insert by line address and more. Here are some of the more useful options to use with sed:\n",
    "\n",
    "|Option|Meaning|\n",
    "|:---|:---|\n",
    "|`-f SCRIPT_FILE`|Add the contents of script-file to the commands to be executed.|\n",
    "|`-r`|Use extended regular expressions in the script.|\n",
    "|`-n`|Suppress automatic printing of pattern space|\n",
    "|`-i[SUFFIX]`|Edit files in place (makes backup if SUFFIX supplied).|\n",
    "\n",
    "And here is some basic usage:\n",
    "\n",
    "- `s`: to substitute\n",
    "- `/../../`: to delimit search and replace patterns (can delimit with #,$,/,...)\n",
    "- `g`: to apply globally\n",
    "- `/../p`: print any line matching `/../`\n",
    "- `/../a`: append after any line matching `/../`\n",
    "- `/../d`: delete any line matching `/../`\n",
    "- `<#>`: specify an address (line number)\n",
    "- `<#1>,<#2>`: specify an address range (from line <#1> to line <#2>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed '1,10p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed -n '1,10p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed 's/\\//\\\\/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed 's&/&\\\\&g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed '/[Rr]e/d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed -n 's&resources_used.*\\([0-9]*:[0-9]*:[0-9]*\\)&\\1&p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed -n 's&resources_used.*\\([0-9]\\{2\\}:[0-9]*:[0-9]*\\)&\\1&p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed -n 's&.*use.* \\([0-9:]\\{8\\}\\)&\\1&p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | sed -rn 's&.*use.* ([0-9:]{8})&\\1&p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369>**AWK**</font>: data-driven language by **A**ho, **W**einberger, and **K**ernighan</u></font>\n",
    "\n",
    "\n",
    "\n",
    "Here are some of the more useful options to use with awk:\n",
    "\n",
    "|Option|Meaning|\n",
    "|:---|:---|\n",
    "|`-f PROGRAM_FILE`|Read the AWK program source from the file PROGRAM_FILE, instead of from the first command line argument.|\n",
    "|`-F FIELD_SEPARATOR`|Use FIELD_SEPARATOR for the input field separator (the value of the FS predefined variable).|\n",
    "|`-v VAR=VAL`|Assign the value VAL to the variable VAR, before execution of the program begins.|\n",
    "|`-r`|Enable the use of interval expressions in regular expression matching.|\n",
    "\n",
    "\n",
    "And here are some of the useful built-in variables:\n",
    "\n",
    "|Variable|Description|\n",
    "|:---|:---|\n",
    "|`FS`|The input field separator, a space by default.|\n",
    "|`RS`|The input record separator, a newline by default.|\n",
    "|`NF`|The number of fields in the current input record.|\n",
    "|`NR`|The total number of input records seen so far.|\n",
    "|`OFMT`|The output format for numbers, \"%.6g\", by default.|\n",
    "|`OFS`|The output field separator, a space by default.|\n",
    "|`ORS`|The output record separator, a newline by default.|\n",
    "|`BEGIN`|Special keyword to indicate beginning of awk program, executes action before processing file(s).|\n",
    "|`END`|Special keyword to indicate end of awk program, executes action after processing all file(s).|\n",
    "|`BEGINFILE`|Special keyword to indicate beginning of each file; executes action before processing each file.|\n",
    "|`ENDFILE`|Special keyword to indicate end of each file; executes action after processing each file.|\n",
    "\n",
    "The basic one-line `awk` command consists of `awk <OPTIONS> '<PATTERN> <ACTION>' <input-file>`. Patterns can include regular expression searches (`/<REGEX>/`) or conditional statements (`NR>4`), along with many more! Actions are usually enclosed in curly braces, and can involve things like displaying specific fields (`{print $3}`) to only print the 3rd field).<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | awk '/resources_used.*[0-9]*:[0-9]*:[0-9]*/ {print}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | awk '/resources_used.*[0-9]*:[0-9]*:[0-9]*/ {print $3}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | awk '/resources_used.*[0-9]*:[0-9]*:[0-9]*/ {split($3,ts,\":\"); print 3600*ts[1]+60*ts[2]+ts[3]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -f $PBS_JOBID | awk '/resources_used.cput/ {split($3,ts,\":\"); cput=3600*ts[1]+60*ts[2]+ts[3]} /resources_used.walltime/ {split($3,ts,\":\"); wclimit=3600*ts[1]+60*ts[2]+ts[3]} END {print 100*cput/wclimit}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369>**=~**</font>: Bash regex</u></font>\n",
    "\n",
    "In bash, the binary operator `=~` is used to compare a string against an extended regular expression; letters in the regular expression can If capture groups are used, the variable `BASH_REMATCH` is an array of the indexed matches.\n",
    "\n",
    "The return values are:\n",
    "\n",
    "- `0` if the string on the LHS matches the regex on the RHS\n",
    "- `2` if the regex on the RHS is syntactically incorrect\n",
    "- `1` otherwise\n",
    "\n",
    "Thus, it can be used in conditional expressions such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [[ \"The quick brown fox\" =~ .\"ui\"(.{4}).*(fox) ]]\n",
    "then\n",
    "  echo \"Return value is $?\"\n",
    "  echo \"Capture group matches are \\\"${BASH_REMATCH[1]}\\\" and \\\"${BASH_REMATCH[2]}\\\"\"\n",
    "  echo \"Note that you could also print the entire matched pattern with \\\"${BASH_REMATCH[0]}\\\"\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFS_OLD=$IFS\n",
    "IFS='\\n$'\n",
    "for line in `qstat -f`\n",
    "do\n",
    "  if [[ $line =~ .*use.*\" \"([0-9:]{8}) ]]\n",
    "  then \n",
    "    echo \"${BASH_REMATCH[1]}\"\n",
    "  fi\n",
    "done\n",
    "IFS=$IFS_OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=B3A369><u>Comparing each approach</u></font>\n",
    "\n",
    "Although `sed` and `awk` are extremely efficient, if you make many separate calls to them, the total time can accummulate and make for an unnecessarily inefficient script. This is a consequence of the shell's need to create many subprocesses and continuously move the programs in and out of memory. As such, if your workflow executes many (hundreds or thousands) regex calls, you may find it more effective to instead use the bash regex operator. As an example, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbsnodes > pbsnodes.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFS_OLD=$IFS\n",
    "IFS='\\n$'\n",
    "time for iter in {1..10}; do for line in $(cat pbsnodes.cache); do echo $line | sed -n 's&.*total_cores = \\([0-9]*\\).*&\\1&p'; done; done > sed.out\n",
    "echo 'sed'\n",
    "IFS=$IFS_OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFS_OLD=$IFS\n",
    "IFS='\\n$'\n",
    "time for iter in {1..10}; do for line in $(cat pbsnodes.cache); do echo $line | awk '/total_cores/ {print $NF}' ; done; done > awk.out\n",
    "echo 'awk'\n",
    "IFS=$IFS_OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFS_OLD=$IFS\n",
    "IFS='\\n$'\n",
    "TOTCOREEXP='.*total_cores = ([0-9]+)'\n",
    "time for iter in {1..10}; do for line in $(cat pbsnodes.cache); do if [[ $line =~ $TOTCOREEXP ]]; then echo \"${BASH_REMATCH[1]}\"; fi; done; done > bash_regex.out\n",
    "echo 'bash regex'\n",
    "IFS=$IFS_OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff sed.out awk.out\n",
    "diff sed.out bash_regex.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=377117>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Example 3: Using AWK and sed to Parse the Output of 'pace-check-queue'\n",
    "</p>\n",
    "\n",
    "- pace-check-queue returns the usual table of nodes within the pace-train queue\n",
    "- The output is piped to awk, which prints the first column (node name), skipping the first 12 lines (output header), and separating each value with a comma\n",
    "- Because the last value is considered its own record, it is followed by the output record seperator, so we pipe the output to sed to substitute only the last comma with a new line character\n",
    "\n",
    "<b>Could this be done with one command (e.g. just awk or sed) instead of the two?</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pace-check-queue pace-training | awk 'BEGIN {ORS=\",\"} /^atl/ {print $1} END {print \"yay\"}' | sed 's&,$&&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pace-check-queue pace-training | awk 'BEGIN {ORS=\",\"} /^atl/ {print $1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=377117>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Example 4: Using AWK and sed to Parse the Output of 'pbsnodes'\n",
    "</p>\n",
    "\n",
    "- pbsnodes provides the node details used for `pace-check-queue`\n",
    "- One of the options allows the data to be output in XML format, which makes for slightly easier parsing\n",
    "- Using a combination of `sed` and `awk`, we can do some neat things!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbsnodes --xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbsnodes --xml | sed 's&<Node>&\\n<Node>&g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pbsnodes --xml | sed 's&<Node>&\\n<Node>&g' | sed -n 's&.*<name>\\([-a-z0-9-]*\\)\\.pace\\.gatech\\.edu</name>.*size=\\([0-9]*\\)kb:\\([0-9]*\\)kb.*&\\1 \\2 \\3&p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pbsnodes --xml | sed 's&<Node>&\\n<Node>&g' \\\n",
    "   | sed -n 's&.*<name>\\([-a-z0-9-]*\\)\\.pace\\.gatech\\.edu</name>.*size=\\([0-9]*\\)kb:\\([0-9]*\\)kb.*&\\1 \\2 \\3&p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  pbsnodes --xml \\\n",
    "    | sed 's&<Node>&\\n<Node>&g' \\\n",
    "    | sed -n 's&.*<name>\\([-a-z0-9-]*\\)\\.pace\\.gatech\\.edu</name>.*size=\\([0-9]*\\)kb:\\([0-9]*\\)kb.*&\\1 \\2 \\3&p' \\\n",
    "    | awk '{sum1+=$2; sum2+=$3; printf \"%40s %5.2f of %6.2f GB used (%5.2f%% available)\\n\",$1\" disk utilization:\",(($3-$2)/1048576),($3/1048576),(100*$2/$3)} END {printf \"Total of %7.2f out of %7.2f GB used (%5.2f%% available)\",((sum2-sum1)/1048576),(sum2/1048576),(100*sum1/sum2)}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbsnodes --xml \\\n",
    "    | sed 's&<Node>&\\n<Node>&g' \\\n",
    "    | sed -n 's&.*<name>\\([-a-z0-9-]*\\)\\.pace\\.gatech\\.edu</name>.*size=\\([0-9]*\\)kb:\\([0-9]*\\)kb.*&\\1 \\2 \\3&p' \\\n",
    "    | awk '\n",
    "          {\n",
    "            sum1+=$2; \n",
    "            sum2+=$3; \n",
    "            printf \"%40s %5.2f of %6.2f GB used (%5.2f%% available)\\n\",$1\" disk utilization:\",\n",
    "            (($3-$2)/1048576),($3/1048576),(100*$2/$3)\n",
    "          }\n",
    "          END {\n",
    "            printf \"Total of %7.2f out of %7.2f GB used (%5.2f%% available)\",\n",
    "            ((sum2-sum1)/1048576),(sum2/1048576),(100*sum1/sum2)\n",
    "          }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWKCMD='\n",
    "{\n",
    "  sum1+=$2; \n",
    "  sum2+=$3; \n",
    "  printf \"%40s %5.2f of %6.2f GB used (%5.2f%% available)\\n\",$1\" disk utilization:\",\n",
    "    (($3-$2)/1048576),($3/1048576),(100*$2/$3)\n",
    "} \n",
    "END {\n",
    "  printf \"Total of %7.2f out of %7.2f GB used (%5.2f%% available)\",\n",
    "  ((sum2-sum1)/1048576),(sum2/1048576),(100*sum1/sum2)\n",
    "}' \n",
    " \n",
    "pbsnodes --xml \\\n",
    "    | sed 's&<Node>&\\n<Node>&g' \\\n",
    "    | sed -n 's&.*<name>\\([-a-z0-9-]*\\)\\.pace\\.gatech\\.edu</name>.*size=\\([0-9]*\\)kb:\\([0-9]*\\)kb.*&\\1 \\2 \\3&p' \\\n",
    "    | awk \"$AWKCMD\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=F95E10>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Hands-On Exercise 1: Stream Manipulation for Efficient Data Processing\n",
    "</p>\n",
    "\n",
    "Use command line tools to look determine the best (i.e. lowest sigma) shape and gap parameters for the filter. These should be stored (and exported!) in the variables SHAPE_TIME and GAP_TIME. There is a right answer, but multiple methods to getting it - do what works for you and makes sense :)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat content/filt.par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export SHAPE_TIME=$(cat content/filt.par | tail -n+2 | sort -k7 -n | awk 'NR==1 {print $1}')\n",
    "export GAP_TIME=$(cat content/filt.par | tail -n+2 | sort -k7 -n | awk 'NR==1 {print $2}')\n",
    "echo \"Shape time is $SHAPE_TIME\"\n",
    "echo \"Gap time is $GAP_TIME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:19.5pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    Beyond Standard Files in Linux\n",
    "</p>\n",
    "\n",
    "<font size=3><u><font color=B3A369><b>Soft/Symbolic and Hard Links</b></font>: Simplifying Paths and Reducing Redundancy</u></font>\n",
    "\n",
    "Sometimes, the same file/directory is needed many times by many applications, or by multiple users. While the data could be copied for each instance, this can introduce problems, mainly:\n",
    "\n",
    "- Any updates to a copy will only be reflected in that copy\n",
    "- Each copy creates data redundancy, requiring more disk space than likely necessary\n",
    "\n",
    "Path links address both of these problems by creating a reference to a path in another location. There are two types of links:\n",
    "\n",
    "- Hard Links: To create a hard link, use `ln <PATH_TO_SOURCE> <PATH_OF_LINK>`\n",
    "    - Hard links associated two or more filenames with the same inode\n",
    "    - Hard links share the same data blocks on disk, but behave as independent files\n",
    "    - Because inodes are partition specific, had links CANNOT span partitions (e.g., TruNAS where home lives cannot have a hard link to GPFS where project lives)\n",
    "    - Hard links CANNOT be created for directories because they create loops\n",
    "    - To remove the source file, all hard links to it must be deleted (in other words, running `rm <PATH_TO_SOURCE>` will remove the file entry in the source location, but `<PATH_OF_LINK>` still can access the file because it was not deleted\n",
    "    - An example use case for hard links is incremental backups with `rsync` ([http://www.mikerubel.org/computers/rsync_snapshots/#Incremental](http://www.mikerubel.org/computers/rsync_snapshots/#Incremental))\n",
    "- Symbolic (or Soft) Links: To create a symbolic link, use `ln -s <PATH_TO_SOURCE> <PATH_OF_LINK>`\n",
    "    - Symbolic links are effectively pointers to a file location\n",
    "    - Symbolic links will create a unique inode entry on the partition on which it resides\n",
    "    - Symbolic links CAN be created for directories\n",
    "    - Removing the source file BREAKS symbolic links\n",
    "    - An example use case for symbolic links is to reduce path name complexity, such as your **data** and **scratch** storage volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo -e \"\\033[1;34mls -ial\\033[m\"\n",
    "ls -ial\n",
    "echo -e \"\\n\\033[1;34mls -ila content\\033[m\"\n",
    "ls -lia content\n",
    "echo -e \"\\n\\033[1;34mls -lid ../Linux102\\033[m\"\n",
    "ls -lid ../Linux102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=377117>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Example 4: Creating a symbolic link to a shared data file\n",
    "</p>\n",
    "\n",
    "- We want to create a symbolic link to a shared data file so that we can treat it like it's actually located here\n",
    "- To make things more exciting, we will randomly pick one of the 5 available files using RANDOM, the built-in bash function for producing random numbers\n",
    "    - By operating modulo 5, we can get a random file name from \"unknown_0.tgz\" through \"unknown_4.tgz\"\n",
    "- We will use this symlink in a later exercise!\n",
    "\n",
    "<b>Can you determine to which file you created a link? (Hint: think about 'file' or 'ls -l')</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln -sfn /storage/home/hpaceice1/shared-classes/materials/linux102/unknown_$((RANDOM%5)).tgz unknown.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>Named Pipes</b></font>: Command Line Shared Memory Streams</u></font>\n",
    "\n",
    "In Linux 101, we learned about the anonymous pipe, `|`, which transfers the STDOUT of one command to STDIN of another. While generally useful, there are limitations to this type of pipe. The data must immediately be consumed by another process, the two commands must run within the same shell, and the two commands must run on the same partition. This is especially problematic for producer-consumer schemes, where two processes run simultaneously in separate shells and exchange data back and forth. In compiled code, the solution is to utilize shared memory, a memory space that is utilized by both programs simultaneously; in addition to the problem of having to write your own code to achieve this, this method has its own challenges in handling the effective read-write synchronization.\n",
    "\n",
    "In Linux, there is a powerful tool called a Named Pipe that can operate outside of compiled applications. A FIFO (first-in, first-out) is a special file type that can ingest data from a producer and provide it to a consumer, even if they're on separate partitions or in different shells. To create a Named Pipe, the command `mkfifo <NAME>` is used. It will look just like a regular file, but now it takes up no disk space and will directly move data from one command to another!\n",
    "\n",
    "Warning: data written to a pipe must be consumed, so producer processes will hang until there is a consumer ingesting the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -f fifo\n",
    "mkfifo fifo\n",
    "ls -l\n",
    "for i in $(seq 10000); do echo $i; done > fifo &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat fifo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>Temporary File System</b></font>: More Explicit Use of Local Memory</u></font>\n",
    "\n",
    "Unfortunately, Named Pipes have the inherent problem that data must be consumed as its written, which can block processes from running. To address this, another option is to use the temporary file system, tmpfs. This is a filesystem based on local memory, and while its use is the same as the regular filesystem, everything written to and read from here is actually in memory, not on a disk. This can help with file I/O immensely, especially if the data is not amenable to caching (although the use of parallel filesystems such as GPFS and Lustre on HPC clusters makes a compelling case to use disk-based filesystems).\n",
    "\n",
    "Often, `/dev/shm` (shared-memory device) is the best way to utilize the temporary file sytem. Data can be written to and read from here just like normal - but if you use it, be sure to clean up after yourself, as this volume is local to the machine and can fill up very quickly!\n",
    "\n",
    "<font size=3><u><font color=B3A369><b>Linux Device Mounts - /dev</b></font>: Super Special Awesome Files in Linux</u></font>\n",
    "\n",
    "The `/dev` directory contains special files for the various devices, which are created during installation. Within this directory are a few really special files:\n",
    "\n",
    "- `/dev/null`: the bit bucket; anything written here will disappear forever; useful if you want to discard the output from commands\n",
    "- `/dev/zero`: provides as many NULL characters as are read from it; the content is formatted, though, so something like `head` with a bit-count should be used to read it (as `cat` can only read unformatted text)\n",
    "- `/dev/random`: a non-deterministic random number generator that uses entropy from system hardware; if no entropy is available, it will wait until more is available before producing additional numbers\n",
    "- `/dev/urandom`: a semi-deterministic random number generator that uses entropy from system hardware; if no entropy is available, it uses a pseudo-random number generator to produce additional numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -c5G /dev/zero | timeout 10 tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -c 13 /dev/urandom | tr -dc A-Za-z0-9[:punct:] ; echo ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>Here Docs and Here Strings</b></font>: On-the-Fly STDIN Utilization</u></font>\n",
    "\n",
    "Sometimes, it is more convenient to redirect variable values or string literals to the STDIN of commands. For example, the `pace-jupyter-notebook` wrapper is a Bash script that uses a built-in template to produce the job script, rather than a separate template file stored elsewhere. To preserve the formatting and utilize run-time determined parameters, it is constructed using a Here Doc.\n",
    "\n",
    "From Linux 101, the `<` operator is used to redirect the contents of a file to STDIN of a command. If, instead, we use `<<TERMINATOR`, the shell will continue reading from the terminal (or if in a script, the subsequent lines) until it encounters the `TERMINATOR` value; this could be a single character, or a sequence such as **EOF**. This operation is called a Here Doc, because a multi-line, formatted document is effectively written between the initial redirect (`<<TERMINATOR`) and the termination sequence (`TERMINATOR`). When writing scripts that use a general format, but can be configured differently, Here Docs are invaluable for passing the content to the command of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awk -F';' '{printf \"%30-s%6.2f\\n\",$2,$3*100/($4+$3)}' <<EOF\n",
    "SEC;Kentucky Wildcats;38;2\n",
    "Big 12;Kansas Jayhawks;32;7\n",
    "ACC;Duke Blue Devils;27;7\n",
    "ACC;North Carolina Tar Heels;32;6\n",
    "Pac-12;UCLA Bruins;19;14\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /opt/pace/bin/pace-jupyter-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, sometimes we want to use the value of a variable or literal string as the input to our command. To redirect this content, we simply add one more `<` to get `<<<STRING`. With this, the string is read directly into STDIN and processed by the command accordingly. The torque submit filter very heavily utilizes Here Strings to efficiently investigate job submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE=\"The quick brown fox jumped over the lazy dog.\"\n",
    "echo $VALUE | sed 's/The \\([[:alpha:]]*\\) \\([[:alpha:]]*\\) fox jumped over the \\([[:alpha:]]*\\) dog./\\1 \\2\\ \\3/'\n",
    "sed 's/The \\([[:alpha:]]*\\) \\([[:alpha:]]*\\) fox jumped over the \\([[:alpha:]]*\\) dog./\\1 \\2\\ \\3/' <<<$VALUE\n",
    "read -r WORD1 WORD2 WORD3 <<<$VALUE && echo $WORD3 $WORD2 $WORD1\n",
    "wc <<<$VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>Process Substitution</b></font>: Redirecting Streams as Files</u></font>\n",
    "\n",
    "Usually we think of I/O redirection as simply piping stdin or stdout between processes using `<` and `>`, respectively. However, sometimes commands expect files for input/output, and cannot process these streams. For instances like this, **process substitution** provides the solution:\n",
    "\n",
    "```\n",
    "Process substitution allows a process’s input or output to be referred to using a filename.\n",
    "```\n",
    "\n",
    "The syntax for process substition is `<(command list)` and `>(command list)` for input and output, respectively. Note, there are no spaces between the `<`/`>` and the opening parentheses, as this would result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awk '{print $1/$2}' <(echo \"10 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:19.5pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    Compiling Code from Source\n",
    "</p>\n",
    "\n",
    "As mentioned in Linux 101, scientific software is usually available in two ways - as a precompiled binary or as source code. Precompiled binaries are pre-built packages\n",
    "\n",
    "<font size=3><u><font color=B3A369><b>Git</b></font>: A Software Repository for Version Control</u></font>\n",
    "\n",
    "According to the `git` man page:\n",
    "\n",
    "```\n",
    "Git is is a fast, scalable, distributed revision control system with an unusually rich command set that provides both high-level operations and full access to internals.\n",
    "```\n",
    "\n",
    "Originally designed for development of the Linux kernel, Git is widely used by teams of programmers to coordinate work, as well as track changes across files. Additionally, a shared Git repository can provide a great way to distribute your code to other researchers. If you write code (in python, bash, java, C, fortran, Bash - any language!) for simulation, analysis, or data manipulation, integrating Git as a part of your workflow can help you immensely!\n",
    "\n",
    "Repository management services like Github (GT provides enterprise access) and Gitlab provide hosting services to share and maintain your repository. This allows you to maintain one global repository, with multiple branches if desired, to proceed with non-linear development \n",
    "\n",
    "Some important commands to use Git are as follows:\n",
    "\n",
    "- `git init`: turn any current repository into a Git repository\n",
    "- `git remote add origin <REMOTE REPOSITORY URL>`: connect your local repository to the remote \n",
    "- `git clone <REMOTE REPOSITORY URL>`: creates a local copy of a remote repository\n",
    "- `git add .`: tells git to track all of the changes in the local repository\n",
    "- `git commit -m \"INITIALS: Comment for commit\"`: adds a useful message to summarize all of the changes that were made\n",
    "- `git push <BRANCH>`: updates the remote repository to reflect the changes made locally and tracked with git add\n",
    "- `git pull`: updates the local repository to reflect the status of the remote repository\n",
    "\n",
    "<font size=3><u><font color=B3A369><b>A Note on Linking Libraries and Environment Variables</b></font></u></font>\n",
    "\n",
    "When compiling and running code, the compiler needs to know where to find shared libraries. The details for the location of shared libraries are actually stored in two separate variables:\n",
    "\n",
    "- `LIBRARY_PATH` tells the compiler where to find the libraries at linkage (i.e. when compiling)\n",
    "- `LD_LIBRARY_PATH` tells the program where to find the libraries at runtime\n",
    "\n",
    "Both of these variables should be set so that the libraries can be found. These variables use the static link (`-L<ABSOLUTE_PATH_TO_LIBRARY`), but when compiling, typically the dynamic link is passed to the compiler (`-l<NAME_OF_LIBRARY>`). If you are unsure of the name of the library (e.g. `fftw` versus `fftw3`), you can run `ldconfig -p | grep <LIBRARY_BASE_NAME>` to see if it is installed, and if so, what is the name of the library.\n",
    "\n",
    "<font size=3><u><font color=B3A369><b>Make</b></font>: Build Automation</u></font>\n",
    "\n",
    "From the `make` man page:\n",
    "\n",
    "```\n",
    "The purpose of the make utility is to determine automatically which pieces of a large program need to be recompiled, and issue the commands to recompile them.\n",
    "```\n",
    "\n",
    "Make is a <b>build automation</b> tool that reads a <b>Makefile</b>, which specifies the details for how to correctly build the target application. The Makefile provides instructions, such as the compiler flags to be used and what libraries should be linked, meaning that you, the user, do not need to remember them each time. Additionally, if only part of the source code is changed, `make` will only build it and any subsequent portions, which improves workflow efficiency by reducing the time spent compiling code.\n",
    "\n",
    "Typically, usage is as simple as updating the <b>Makefile</b> to reflect the local software environment (if not in usual variables such as `LD_LIBRARY_PATH`, `LIBRARY_PATH`, etc.) and running `make` (and `make install` if this will install the project somewhere else). If an error occurs, and the build needs to be restarted, simply run `make clean` to purge any problematic components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=F95E10>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Hands-On Exercise 2: Building Code from Source with git and Make\n",
    "</p>\n",
    "    \n",
    "Clone the repository from [https://gitlab.com/apjezghani/analyze](https://gitlab.com/apjezghani/analyze), load the appropriate modules, and build the project.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd analyze\n",
    "module load gcc/8.3.0 fftw root\n",
    "make clean\n",
    "make\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -R analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:19.5pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    File Management with Compression and Archiving\n",
    "</p>\n",
    "\n",
    "Compression and archival of data has many benefits for researchers:\n",
    "\n",
    "- Reduced data volume, allowing more data to be stored on disk\n",
    "- And reduced number of inodes, preventing issues with number of file/directory quotas\n",
    "- Smaller files can be transferred more quickly (for example, from network storage like on PACE)\n",
    "- Reduces data structure complexity, as multiple files can be compressed to a single archive\n",
    "\n",
    "The trade-off, however, is that CPU time must be spent packing/unpacking the data, which can impact performance. The trick is to find the right balance for individual needs - what may work for one problem might not be applicable for another.\n",
    "\n",
    "With regards to compression, there are two types of compression: lossy and lossless\n",
    "\n",
    "- <b>Lossy Compression</b> is fast and can achieve very high compression ratios (95%+ is quite common), but it introduces \"noise\" (random errors) into the data, so it can never be reconstructed perfectly. Common examples include JPEG, MP3, and MP4 files; more specialized examples for scientific research are SZ and ZFP.\n",
    "- <b>Lossless Compression</b> is slower and typically achieves lower compression ratios (50-75% is usual), but the files can be reconstructed exactly. Common examples include PNG, FLAC, and ZIP, however, domain specific tools such as GeCo for bioinformatics can improve both compression rate and ratio.\n",
    "\n",
    "<img src=\"img/lossy_compression.png\" alt=\"Bash indicating that the job is now running in the background.\" width=\"75%\" align=\"center\">\n",
    "\n",
    "<font size=3><u><font color=B3A369><b>Lossless compression utilities</b></font>: zip, gzip, bzip2, and xz</u></font>\n",
    "\n",
    "A variety of utilities are available as \"standard\" on most Linux systems (they don't need to be built from source), and they each serve a purpose. The zip format used on Windows is also available on Linux, but for many reasons other compression utilities are preferred. The table below lists the standard compression utilities, and some general properties (but note - these are very generalized statements, and the individual use case should be considered before picking the utility to use):\n",
    "\n",
    "|Utility|Algorithm|Archives?|Compression Ratio|Compression Speed|Decompression Speed|\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|\n",
    "|zip|DEFLATE|Yes|2x-3x|Very Fast|Very Fast|\n",
    "|gzip|DEFLATE|No|2x-3x|Very Fast|Very Fast|\n",
    "|bzip2|Burrows-Wheeler + Huffman|No|3x-4x|Fast|Very Slow|\n",
    "|xz|LZMA|No|5x-7x|Very, Very Slow|Slow|\n",
    "\n",
    "From the above table, it would seem that the only difference between `zip` and `gzip` is that the former can archive (compress multiple files at once) while the latter cannot. However, an important detail is that when `zip` is used to archive, each file is compressed individually - thus it cannot benefit from similarities between each file! In other words, compressing and archiving many similar files with `zip` will often result in lower efficiency than the same operation with `gzip`+`tar`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><u><font color=B3A369><b>Archiving with tar</b></font>: the <b>t</b>ape <b>ar</b>chive utility</u></font>\n",
    "\n",
    "As mentioned in Linux 101, `tar` is used for archival (packaging many directories and files into a single container), and can be used with the above utilities to compress the archive (`-zc` compresses with `gzip` and `-zx` decompresses with `gunzip`). Some of the more advanced options for `tar` include:\n",
    "\n",
    "- `-j`: filter compression/decompression through `bzip2`\n",
    "- `-J`: filter compression/decompression through `xz` (note: this replaces the deprecated `--lzma` option)\n",
    "- `-I <UTILITY>`: filter compression/decompression through `<UTILITY>` (note: the utility must accept the `-d` option to find the differences between the filesystem and archive)\n",
    "- `-t`: list the contents of the archive (use with a compression utility flag if needed)\n",
    "- `-O`: write the extracted data to STDOUT instead of to the filesystem\n",
    "- `--to-command=<COMMAND>`: pipe the extracted data to STDIN of `<COMMAND>`\n",
    "\n",
    "<font color=377117>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Example 5: Comparing compression with different data types\n",
    "</p>\n",
    "\n",
    "- To highlight the differences in the standard command line compression utilities, try running the test script **compress-demo.sh**, which analyzes performance on some small files for illustrative purposes\n",
    "- The first command allows us to see what the script does\n",
    "    - For each data type file and compressor combination, time compression and decompression, and report the compressed size\n",
    "- The second command actually runs the test - but be sure to run it twice to get results that make sense!\n",
    "\n",
    "<b>What general conclusions can you draw regarding the differences between the compression utilities? Why do you think we had to run the second command twice?</b>\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat content/compress-demo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./content/compress-demo.sh #run this command twice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=F95E10>\n",
    "<p style=\"font-size:14pt; text-decoration:underline; font-weight:bold\">\n",
    "    Hands-On Exercise 3: Putting it All Together\n",
    "</p>\n",
    "\n",
    "Use command line tools to create a PBS script that will:\n",
    "\n",
    "- Set the appropriate PBS directives for the job:\n",
    "    - &num;PBS -q pace-train\n",
    "    - &num;PBS -l walltime=15:00\n",
    "    \n",
    "- Load the prerequisite modules for the **analyze** program\n",
    "- Use the results from Exercise 1 (stored in the variables SHAPE_TIME and GAP_TIME) that give the best resolution (smallest SIGMA) as command line arguments to the **analyze** program\n",
    "- Unpack the linked tarball in memory and run the **analyze** program on the data\n",
    "\n",
    "Submit the job via qsub and wait for results!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -ztf unknown.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat <<EOF > script.pbs\n",
    "#PBS -q pace-ice\n",
    "#PBS -l mem=8gb\n",
    "\n",
    "cd \\$PBS_O_WORKDIR\n",
    "pwd\n",
    "module load gcc/8.3.0 fftw root\n",
    "\n",
    "./analyze/bin/analyze $SHAPE_TIME $GAP_TIME <(tar -Ozxf unknown.tgz)\n",
    "\n",
    "EOF\n",
    "\n",
    "qsub script.pbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat script.pbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:19.5pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    Your Feedback is Valued!\n",
    "</p>\n",
    "\n",
    "While the above code runs, please take the time to fill out this brief questionnaire. These answers provide useful feedback to better tailor the course content to your needs!\n",
    "\n",
    "<center><font size=4><a href=https://b.gatech.edu/2Fwx1PB>https://b.gatech.edu/2Fwx1PB</a></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:19.5pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    Checking Your Results\n",
    "</p>\n",
    "\n",
    "When your analysis job has finished (~2-3 minutes), run the command in the cell below to display an image of of the spectrum. Compare it to the known spectra below - which isotope do you think you had?\n",
    "\n",
    "<img src=\"img/choices.png\" alt=\"Possible nuclear isotopes for measured spectra\" width=\"100%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display < spectrum.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:19.5pt; text-decoration:underline; font-weight:bold; color:#003057\">\n",
    "    Some Useful Links!\n",
    "</p>\n",
    "\n",
    "* [https://explainshell.com/](https://explainshell.com/): Breaks down shell commands and options according to utility man page\n",
    "* [https://www.regextester.com/111539](https://www.regextester.com/111539): A regular expression tester (personally I still prefer fumbling through the command line).\n",
    "* [An Introduction to Regular Expressions](https://learning.oreilly.com/library/view/an-introduction-to/9781492082569/): A textbook detailing the use of regular expressions for pattern matching.\n",
    "* [Learning AWK Programming](https://learning.oreilly.com/library/view/learning-awk-programming/9781788391030/): A textbook all about AWK!\n",
    "* [sed & awk Pocket Reference](https://learning.oreilly.com/library/view/sed-and-awk/0596003528/ch01.html): A brief overview for the usage of sed and AWK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
